{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Upenn specimen\n",
    "\n",
    "Convert HDF5 file to .nii.gz files for nnUNet inference\n",
    "\n",
    "<!-- ![img] (chunking) -->\n",
    "<img src=\"chunking.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AERB\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "# Import python library\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "from skimage import exposure\n",
    "import os\n",
    "import nibabel as nib\n",
    "import tifffile as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions: \n",
    "\n",
    "`generate_niigz`: Input 3D numpy img, write .nii.gz file to destination \n",
    "\n",
    "`writetiff`: Input 3D numpy img, write .tiff file to destination\n",
    "\n",
    "`chunk`: Loop through the channels, and calculate the coordinate of chunked blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_niigz(img, output_path):\n",
    "    \"\"\"\n",
    "    Map to 8 bit\n",
    "    \"\"\"\n",
    "\n",
    "    img = exposure.rescale_intensity(img,\n",
    "                               in_range=(np.min(img), np.percentile(img,99.99)), \n",
    "                               out_range=np.uint8)\n",
    "\n",
    "    # Create the NIfTI object from the 3D array and save the NIfTI object\n",
    "    nib.Nifti1Image(img.astype(np.uint8), None).to_filename(output_path)\n",
    "\n",
    "def writetiff(img, output_path):\n",
    "    \"\"\"\n",
    "    Keep 16 bit, less compression\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Saving TIFF...\")\n",
    "    with tf.TiffWriter(output_path) as tif:\n",
    "        for i in range(len(img)):\n",
    "            tif.write(img[i], contiguous=True)\n",
    "\n",
    "\n",
    "def chunk(h5path, overlap = 0.25, blocksize = 1024, ftype = \".nii.gz\", res=\"0\", nlvl = \"\"):\n",
    "    \"\"\"\n",
    "    Params: \n",
    "    - h5path\n",
    "    - savepath\n",
    "    - overlap_percentage (can be 0)\n",
    "    - ftype: \".nii.gz\" or \".tiff\"\n",
    "    - nlvl: how many levels from the top\n",
    "    - res: resolution, use \"0\"\n",
    "    - nblk (for chunking by specifying number of blocks)\n",
    "    - blocksize (for chunking by specifying blocksize)\n",
    "    \"\"\"\n",
    "    savepath = os.path.dirname(h5path) + os.sep + \"nnunet\"\n",
    "\n",
    "    if not os.path.exists(savepath):\n",
    "        os.mkdir(savepath)\n",
    "\n",
    "    chan = [(\"s01\", \"0001\"),\n",
    "            (\"s00\", \"0000\" )]\n",
    "\n",
    "    with h5.File(h5path, 'r') as f:\n",
    "        img_shape = f['t00000'][chan[0][0]][res]['cells'].shape\n",
    "    f.close()\n",
    "\n",
    "    x_num_blocks = int((img_shape[1] - overlap * blocksize) // ((1 - overlap) * blocksize))\n",
    "    y_num_blocks = int((img_shape[2] - overlap * blocksize) // ((1 - overlap) * blocksize))\n",
    "\n",
    "    # Loop through each block\n",
    "    for ch in chan:\n",
    "        for i in range(x_num_blocks):\n",
    "            for j in range(y_num_blocks):\n",
    "                # Calculate start and end indices for x and y dimensions with overlap\n",
    "                x_start = int(i * blocksize * (1-overlap))\n",
    "                x_end = int(x_start + blocksize)\n",
    "                y_start = int(j * blocksize * (1-overlap))\n",
    "                y_end = int(y_start + blocksize)\n",
    "\n",
    "                with h5.File(h5path, 'r') as f:\n",
    "                    if nlvl == \"\":\n",
    "                        img = f['t00000'][ch[0]][res]['cells'][:, x_start:x_end, y_start:y_end].astype(np.uint16)\n",
    "                    else:\n",
    "                        img = f['t00000'][ch[0]][res]['cells'][:nlvl, x_start:x_end, y_start:y_end].astype(np.uint16)\n",
    "                    if ftype == \".nii.gz\":\n",
    "                        img = np.moveaxis(img,0,2)\n",
    "                f.close()\n",
    "\n",
    "                # Prepare filename for the block\n",
    "                # fname = os.path.basename(os.path.dirname(h5path)) + f\"_blk_{i}_{j}_\" + ch[1] + ftype\n",
    "                fname = h5path.split(\"-23_\")[1].split(\"_well\")[0] + f\"_blk_{i}_{j}_\" + ch[1] + ftype\n",
    "                fpath = savepath + os.sep + fname\n",
    "\n",
    "                # # Print the indices for verification (you can remove this if not needed)\n",
    "                if ftype == \".nii.gz\":\n",
    "                    generate_niigz(img, fpath)\n",
    "                else:\n",
    "                    writetiff(img, fpath)\n",
    "\n",
    "                print(x_start, x_end, y_start, y_end)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the fused_corrected .h5 paths for all upenn files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = r\"W:\\UPenn_Clinical\" \n",
    "fused_corrected_h5_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(root_folder):\n",
    "    if \"fused.h5\" in files and \"fused_corrected1.h5\" in files:\n",
    "        if root.split(os.sep)[2][:5] == \"OTLS4\":\n",
    "            try:\n",
    "                subfolder = root.split(os.sep)[1].split(os.sep)[0]\n",
    "                if subfolder!= \"old_datasets\":\n",
    "                    fused_corrected_h5_paths.append(os.path.join(root, \"fused_corrected1.h5\"))\n",
    "            except IndexError: \n",
    "                fused_corrected_h5_paths.append(os.path.join(root, \"fused_corrected1.h5\"))\n",
    "\n",
    "fused_corrected_h5_paths.pop(0) # get rid of the repeated 'W:\\\\UPenn_Clinical\\\\OTLS4_NODO_1-17-23_AFM004_well_5\\\\fused_corrected1.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk all the upenn specimen, it will take a longgg time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fused_corrected_h5_paths)):\n",
    "    chunk(fused_corrected_h5_paths[i], \n",
    "          overlap = 0.25, \n",
    "          blocksize = 1024, \n",
    "          ftype = \".nii.gz\", \n",
    "          res=\"0\", \n",
    "          nlvl = \"\")\n",
    "    print(\"chunking: \", fused_corrected_h5_paths[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, manually specify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1024 0 1024\n",
      "0 1024 768 1792\n",
      "0 1024 1536 2560\n",
      "0 1024 2304 3328\n",
      "0 1024 3072 4096\n",
      "768 1792 0 1024\n",
      "768 1792 768 1792\n",
      "768 1792 1536 2560\n",
      "768 1792 2304 3328\n",
      "768 1792 3072 4096\n",
      "1536 2560 0 1024\n",
      "1536 2560 768 1792\n",
      "1536 2560 1536 2560\n",
      "1536 2560 2304 3328\n",
      "1536 2560 3072 4096\n",
      "2304 3328 0 1024\n",
      "2304 3328 768 1792\n",
      "2304 3328 1536 2560\n",
      "2304 3328 2304 3328\n",
      "2304 3328 3072 4096\n",
      "3072 4096 0 1024\n",
      "3072 4096 768 1792\n",
      "3072 4096 1536 2560\n",
      "3072 4096 2304 3328\n",
      "3072 4096 3072 4096\n",
      "0 1024 0 1024\n",
      "0 1024 768 1792\n",
      "0 1024 1536 2560\n",
      "0 1024 2304 3328\n",
      "0 1024 3072 4096\n",
      "768 1792 0 1024\n",
      "768 1792 768 1792\n",
      "768 1792 1536 2560\n",
      "768 1792 2304 3328\n",
      "768 1792 3072 4096\n",
      "1536 2560 0 1024\n",
      "1536 2560 768 1792\n",
      "1536 2560 1536 2560\n",
      "1536 2560 2304 3328\n",
      "1536 2560 3072 4096\n",
      "2304 3328 0 1024\n",
      "2304 3328 768 1792\n",
      "2304 3328 1536 2560\n",
      "2304 3328 2304 3328\n",
      "2304 3328 3072 4096\n",
      "3072 4096 0 1024\n",
      "3072 4096 768 1792\n",
      "3072 4096 1536 2560\n",
      "3072 4096 2304 3328\n",
      "3072 4096 3072 4096\n",
      "chunking:  W:\\UPenn_Clinical\\OTLS4_NODO_8-14-23_AFM014_well_5\\fused_corrected1.h5\n"
     ]
    }
   ],
   "source": [
    "h5path = r\"W:\\UPenn_Clinical\\OTLS4_NODO_8-14-23_AFM014_well_5\\fused_corrected1.h5\"\n",
    "# W:\\UPenn_Clinical\\OTLS4_NODO_8-14-23_AFM014_well_5\n",
    "overlap = 0.25  \n",
    "blocksize = 1024\n",
    "ftype = \".nii.gz\"\n",
    "nlvl = \"\"\n",
    "res = \"0\"\n",
    "chunk(h5path, \n",
    "        overlap = overlap, \n",
    "        blocksize = blocksize, \n",
    "        ftype = ftype, \n",
    "        res=res, \n",
    "        nlvl = nlvl)\n",
    "print(\"chunking: \",h5path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.7 ('Sarah')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37ac82985e9ebe1da2f8485186222d79911c1060e46fa5b5cef826fba9d0c0d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
